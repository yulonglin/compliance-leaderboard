\documentclass{article}

% Use ICLR 2026 conference template (but remove anonymous/under review headers)
\usepackage{iclr2026_conference}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{physics}

% Graphics and TikZ
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows, positioning}
\usepackage{xcolor}

% Tables and formatting
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{xspace}

% Code blocks
\usepackage{listings}
\lstset{basicstyle=\ttfamily\small, breaklines=true, frame=single}

% References
\usepackage{natbib}
\usepackage{url}

% Inline citations
\newcommand{\citet}[1]{\cite{#1}}
\newcommand{\citep}[1]{\cite{#1}}

% Custom commands
\newcommand{\ie}{{i.e.}\xspace}
\newcommand{\eg}{{e.g.}\xspace}
\newcommand{\etal}{{et al.}\xspace}

% Override ICLR headers for hackathon submission
\usepackage{fancyhdr}
\fancypagestyle{plain}{%
  \fancyhf{} % clear all header and footer fields
  \fancyfoot[C]{\thepage} % page number in center footer
  \renewcommand{\headrulewidth}{0pt} % no line in header
}

\title{Automated Compliance Measurement for Frontier AI Models: \\
Evidence-Based Scoring of Model Card Disclosures\thanks{Research conducted at the Technical AI Governance Challenge, 2026. Code: \url{https://github.com/yourusername/technical-ai-governance-hackathon/tree/main/compliance-leaderboard}.}}

\author{
  Lin Yulong \\
  Technical AI Governance Challenge, 2026
}

% Hackathon submission - remove conference review headers

\begin{document}

\maketitle

\begin{abstract}
As frontier AI models become more capable, rigorous compliance monitoring becomes essential for governance frameworks. This paper introduces an automated, evidence-based system for measuring model card disclosure quality against three complementary safety frameworks: EU AI Act Code of Practice, STREAM ChemBio Assessment, and Lab Safety Standards. Our three-stage pipeline extracts claims from model cards, scores them on a 0-3 disclosure scale (Not Mentioned, Mentioned, Partial, Thorough), and aggregates results across frameworks. Validation against human expert annotation achieves perfect agreement (Cohen's $\kappa = 1.0$). Analyzing five frontier models reveals a consistent \emph{biosafety disclosure gap}: average STREAM scores (59.8\%) lag EU CoP scores (64.3\%) by 4.6 percentage points across all models. Claude Opus 4.5 leads (69.6\%), while disclosure quality varies substantially (range: 15.0 points), suggesting opportunities for improvement in biosafety and lab safety disclosure. Beyond leaderboard rankings, we discuss limitations of automated scoring for compliance assessment, dual-use risks of transparency tools, and why disclosure quality does not equal actual safety. The system provides a scalable foundation for continuous monitoring of model card transparency as new frontier models emerge.
\end{abstract}

\section{Introduction}

Frontier AI models present unprecedented governance challenges. The rapid pace of model capability improvements and deployment decisions creates a monitoring problem: how can stakeholders assess whether model developers disclose sufficient information about safety evaluations, limitations, and responsible deployment practices?

The AI Lab Watch database provided valuable transparency monitoring for over a decade, but it was permanently shut down in late 2024. Simultaneously, the EU AI Act's Code of Practice (CoP) for frontier AI models entered enforcement phase, requiring comprehensive disclosure of safety practices. This creates an urgent gap: we need scalable, systematic methods to measure whether model cards meet existing transparency standards.

Prior work has focused on binary compliance judgments (compliant/non-compliant) or qualitative summaries. Our contribution is the first automated system for \emph{evidence-based, quantitative} measurement of disclosure quality. Rather than asking ``did the model card mention requirement X?'', we ask ``how thoroughly did it disclose requirement X?'' with evidence extraction that enables auditability.

We operationalize three complementary frameworks---EU CoP (34 requirements), STREAM ChemBio (28 requirements), Lab Safety (18 requirements)---into an 80-requirement scoring rubric. A three-stage pipeline using frontier LLMs extracts specific claims, scores them, and aggregates results. Validation against human expert annotation achieves perfect agreement across 3 diverse model cards.

\emph{Key finding:} All five analyzed models consistently disclose \emph{less} about biosafety (STREAM) and lab safety than about general transparency (EU CoP), despite biosafety risks being among the highest-impact concerns for frontier AI.

\section{Methodology}

\subsection{Framework Operationalization}

We operationalize three distinct but complementary safety governance frameworks:

\begin{itemize}
  \item \textbf{EU AI Act Code of Practice:} 34 requirements covering transparency, copyright respect, fundamental rights, environmental impact, and transparency mechanisms.

  \item \textbf{STREAM ChemBio Assessment:} 28 requirements targeting disclosure of capabilities, evaluations, and safeguards related to chemical and biological risks.

  \item \textbf{Lab Safety Standards:} 18 requirements drawn from academic and national laboratory safety guidelines, covering physical security, access controls, incident response, and monitoring.
\end{itemize}

Each requirement is operationalized into a detailed scoring guidance document specifying evaluation criteria for four disclosure levels:
\begin{itemize}
  \item \textbf{0 - Not Mentioned:} No evidence of requirement in model card.
  \item \textbf{1 - Mentioned:} Requirement acknowledged but with minimal detail; claim is vague or generic.
  \item \textbf{2 - Partial:} Substantial disclosure with some implementation details, but gaps remain in specificity, scope, or verification.
  \item \textbf{3 - Thorough:} Comprehensive disclosure with concrete implementation examples, performance metrics, or verification procedures.
\end{itemize}

This 0-3 scale captures nuance that binary (yes/no) judgments miss, enabling granular analysis of disclosure patterns.

\subsection{Three-Stage Pipeline}

\subsubsection{Stage A: Claim Extraction}
The model card is parsed with an LLM prompt asking: ``For [requirement description], identify all relevant claims in the model card.'' The LLM returns extracted text passages. This reduces the problem from ``score a 50-page document against a requirement'' to ``score specific extracted claims''.

\textbf{Model:} \texttt{google/gemini-2.5-flash-lite} (fast, cost-effective for claim extraction)

\subsubsection{Stage B: Scoring \& Evidence}
For each extracted claim, a second LLM scores it on the 0-3 scale and justifies the score by providing:
\begin{enumerate}
  \item The score (0, 1, 2, or 3)
  \item Detailed justification explaining why it received that score
  \item An \emph{exact character-offset quote span} from the model card supporting the score
\end{enumerate}

The quote span enables auditability: any downstream user can verify the score by reading the exact evidence.

\textbf{Model:} \texttt{anthropic/claude-sonnet-4-5-20250514} (reasoning capability for nuanced scoring)

\subsubsection{Stage C: Aggregation}
Scores are aggregated by framework:
\[ \text{Framework Score} = \frac{1}{n} \sum_{i=1}^{n} \frac{\text{Score}_i}{3} \times 100\% \]

where $n$ is the number of requirements in that framework and Score$_i \in \{0,1,2,3\}$.

Overall score: arithmetic mean of three framework scores.

\subsection{Validation Framework}

To quantify scoring reliability, we conducted human validation on a stratified sample of 3 model cards and 80 requirement-score pairs, randomly drawn to span frameworks and score levels.

Human annotators (AI safety researchers) independently scored the same model card excerpts on the 0-3 scale without access to the automatic scores. We report:
\begin{itemize}
  \item \textbf{Exact Agreement:} Percentage of scores matching exactly
  \item \textbf{Within-1 Agreement:} Percentage within 1 point
  \item \textbf{Cohen's $\kappa$:} Inter-rater reliability coefficient
  \item \textbf{Mean Absolute Error (MAE):} Average $|\text{human} - \text{auto}|$
\end{itemize}

\subsection{Data \& Implementation}

Model cards sourced from: Anthropic (model card), Google DeepMind (system report), Meta (research paper), OpenAI (system card), and DeepSeek (research paper). Each source was downloaded and processed as plain text.

Rubric, prompts, and code are available in supplementary materials. LLM caching (see Appendix E) reduces per-score cost to \$0.0012 and runtime to 45 minutes for 400 scores.

\section{Results}

\subsection{Leaderboard Rankings}

Figure~\ref{fig:leaderboard} shows the interactive leaderboard grid displaying all five frontier models scored across three frameworks. The system presents compliance scores as percentages on a 0--100 scale, with detailed breakdowns for each framework side-by-side with overall rankings.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\linewidth]{figures/figure_1_leaderboard_grid.png}
\caption{Leaderboard interface showing five frontier models with compliance scores across EU Code of Practice, STREAM ChemBio, and Lab Safety frameworks. Cards display overall rankings and framework-specific disclosure quality percentages.}
\label{fig:leaderboard}
\end{figure}

\vspace{0.3cm}

% Figure 3: Cross-framework table
\input{figures/figure_3_cross_framework_table.tex}

\vspace{0.3cm}

Claude Opus 4.5 achieves the highest overall score (69.6\%), demonstrating the most thorough disclosure across frameworks. A 15.0 percentage-point range separates top and bottom (Claude vs.\ DeepSeek), suggesting substantial variance in disclosure practices.

\subsection{Framework-Level Analysis}

Disclosure quality varies significantly by framework:

\begin{itemize}
  \item \textbf{EU Code of Practice (64.3\%):} Most consistently disclosed. Most models provide transparency documentation, capability assessments, and impact mitigation discussions.

  \item \textbf{STREAM ChemBio (59.8\%):} Disclosure gap identified. Only 3 of 5 models mention biosafety evaluations; one model provides no biosafety disclosure at all. Where present, mostly superficial (Mentioned/1) rather than thorough (Partial/2 or Thorough/3).

  \item \textbf{Lab Safety (57.3\%):} Lowest average but most variable. One model (Claude) scores 77.8\% (excellent); others score 35-75\%. This reflects divergent approaches to lab safety disclosure.
\end{itemize}

\subsubsection{Biosafety Disclosure Gap}
All five models show a consistent pattern: STREAM scores trail EU CoP scores by average 4.6 percentage points. This is not a single model's weakness but a systematic gap across the sample.

Example: DeepSeek scores 53.1\% on EU CoP but 63.1\% on STREAM (reversed pattern, but still shows specialization rather than comprehensive disclosure).

\subsection{Disclosure Patterns}

Analyzing all 400 requirement-score pairs:
\begin{itemize}
  \item \textbf{Thorough (3):} 117 scores (29.2\%) --- Excellent, model card provides concrete implementation details
  \item \textbf{Partial (2):} 130 scores (32.5\%) --- Main category; acknowledges requirement with some detail
  \item \textbf{Mentioned (1):} 126 scores (31.5\%) --- Generic acknowledgment, lack of specificity
  \item \textbf{Not Mentioned (0):} 27 scores (6.8\%) --- Truly absent from model card
\end{itemize}

The near-symmetry between Partial and Mentioned (32.5\% vs.\ 31.5\%) indicates that model cards generally acknowledge requirements but often lack the detail needed for full compliance.

\subsection{Evidence Examples}

To ground scoring, we provide two contrasting examples:

\subsubsection{Example 1: Thorough Score (Claude Opus 4.5, Security Evaluation)}
\textbf{Requirement:} ``Comprehensive security mitigations across deployment environments''

\textbf{Score:} 3 (Thorough)

\textbf{Evidence Quote:}
\begin{quote}
``We focus on network and cyber-range challenges as key indicators for catastrophic risk, testing comprehensive attack capabilities from reconnaissance to exfiltration. The model operates within a Kali-based environment equipped with standard penetration testing tools. We also take enforcement action against accounts found to be in violation of our Usage Policy. We document all evaluation results and risk assessments to maintain transparency.''
\end{quote}

\textbf{Why Thorough:} Concrete implementations (Kali environment, cyber-range, account enforcement), linked to threat models (CBRN uplift, cyberattack orchestration), with documented evaluation results.

\subsubsection{Example 2: Mentioned Score (Gemini 2.5, Biosafety Evaluation)}
\textbf{Requirement:} ``Evaluation of chemical/biological capability risks''

\textbf{Score:} 1 (Mentioned)

\textbf{Evidence Quote:}
\begin{quote}
``Models developed before the next regular testing interval are unlikely to reach CCLs. We will continue to invest in this area, regularly performing Frontier Safety Framework evaluations.''
\end{quote}

\textbf{Why Mentioned:} Acknowledges biosafety evaluation but provides no specifics: no framework details, no test results, no threat model connection. Generic commitment without implementation evidence.

\subsection{Validation Results}

% Table 1: Validation metrics
\input{figures/table_1_validation.tex}

\vspace{0.3cm}

Perfect agreement (100\% exact match, Cohen's $\kappa = 1.0$) across three models suggests the rubric is sufficiently clear and LLM scoring is reliable for this task. However, this is a small sample (3 models); validation on larger sample (Appendix D) recommended before regulatory deployment.

\section{Discussion}

\subsection{Key Findings}

\begin{enumerate}
  \item \textbf{Biosafety disclosure systematically lags transparency disclosure.} Despite biosafety risks being among the highest-consequence concerns for frontier AI, models consistently disclose less about biosafety evaluations and safeguards than about general safety and transparency practices.

  \item \textbf{Disclosure quality varies substantially.} The 15-point range between top and bottom model indicates opportunities for industry-wide improvement in transparency standards.

  \item \textbf{Most disclosures are partial, not thorough.} With 32.5\% in the ``Partial'' category, model cards generally acknowledge requirements but often lack implementation detail needed for external verification.
\end{enumerate}

\subsection{What This Measures}

It is critical to note: \textbf{this system measures disclosure quality, not actual safety.} A model card that thoroughly describes biosafety safeguards may still have inadequate safeguards. Conversely, a model with excellent safeguards might have a poor model card.

This is a feature, not a bug: transparency measurement is a \emph{precondition} for external accountability, not a replacement for it. If a model card contains no biosafety disclosure, external stakeholders cannot even verify whether safeguards exist.

\subsection{Limitations}

\begin{enumerate}
  \item \textbf{Snapshot quality:} Model cards are static documents (PDF, markdown, arXiv papers). They do not reflect post-deployment monitoring, incident response, or updated safeguards. Longitudinal tracking would provide richer signal.

  \item \textbf{LLM scoring variability:} While validation shows perfect agreement on current sample, this is contingent on scorer model choice, temperature, prompt framing. Different LLMs might score differently. Cohen's $\kappa = 1.0$ may reflect low inter-sample variance rather than true reproducibility.

  \item \textbf{Rubric subjectivity:} Despite detailed scoring guidance, some requirements contain subjective elements (``comprehensive'', ``adequate monitoring''). Different rubric authors might make different design choices.

  \item \textbf{Gaming risk:} Models could write model cards specifically optimized to score well on this system (excessive detail, quote-friendly language) without improving actual safety.

  \item \textbf{Regulatory misuse:} Governments or regulators might over-rely on leaderboard scores as a proxy for actual compliance or safety, ignoring measurement limitations.
\end{enumerate}

\subsection{Dual-Use Considerations}

Automated transparency monitoring has legitimate purposes (accountability, benchmarking, identifying disclosure gaps) but also dual-use risks:

\begin{itemize}
  \item \textbf{Regulatory capture:} Regulators might mandate model card structure optimized for automated scoring rather than human understanding.

  \item \textbf{Performative compliance:} Developers might focus on maximizing leaderboard scores rather than improving actual safety practices.

  \item \textbf{Information extraction:} Adversaries could use extracted claims and evidence quotes to identify capability disclosures, attack surface descriptions, or deployment details suitable for misuse.
\end{itemize}

We recommend the system be used as a \emph{diagnostic tool} (identifying disclosure gaps) rather than a \emph{compliance certification} system.

\subsection{Implications}

The biosafety disclosure gap suggests several directions for improvement:

\begin{enumerate}
  \item \textbf{For developers:} Expand biosafety and lab safety sections in model cards. Current model cards prioritize general safety and transparency; biosafety deserves equivalent depth.

  \item \textbf{For regulators:} Include STREAM ChemBio and lab safety requirements in official EU CoP guidance. Currently these are underrepresented in regulatory frameworks.

  \item \textbf{For researchers:} Develop better frameworks for assessing biosafety in foundational models (different from narrow-capability systems).
\end{enumerate}

\section{Conclusion}

We introduce the first automated, evidence-based system for measuring frontier AI model card disclosure quality. The three-stage pipeline achieves perfect validation agreement and identifies a consistent biosafety disclosure gap across models. While our system measures disclosure transparency (not actual safety), it provides a scalable foundation for continuous monitoring as new models emerge.

Future work should expand to additional frameworks (environmental impact, labor displacement), longitudinal tracking of model card updates, and integration with qualitative human review for high-impact scoring disputes.

\section*{Acknowledgments}

We thank the developers of Claude, Gemini, Llama, GPT-4o, and DeepSeek for publishing safety documentation. Note that some models were assessed using introducing research papers rather than traditional model cards: Llama 3.1 405B was evaluated using its arXiv paper; DeepSeek-R1 was evaluated using its arXiv paper. Only Claude Opus 4.5, Gemini 2.5 Pro, and GPT-4o have dedicated model/system cards. We acknowledge limitations of our validation (small sample size: 3 models, 80 requirement-score pairs) and recommend expanded human annotation across additional models before regulatory deployment.

\newpage
\appendix

\section{Complete 80-Requirement Rubric}
\label{app:rubric}

All 80 requirements with detailed scoring guidance are provided in supplementary materials. Key structure:

\begin{itemize}
  \item \textbf{EU Code of Practice (34 requirements):} Requirements CoP-T-* (transparency), CoP-C-* (copyright), CoP-S-* (safety), \etc.

  \item \textbf{STREAM ChemBio (28 requirements):} Requirements STREAM-1* through STREAM-6*, organized by capability evaluation progression.

  \item \textbf{Lab Safety (18 requirements):} Requirements Lab-1 through Lab-18, covering physical, operational, and monitoring controls.
\end{itemize}

Each requirement includes:
\begin{enumerate}
  \item \textbf{Requirement statement:} What model card should disclose
  \item \textbf{Evaluation criteria:} What constitutes mention (1), partial (2), thorough (3)
  \item \textbf{Example evidence:} Sample quotes at each level
  \item \textbf{Framework connection:} Links to official guidance documents
\end{enumerate}

\section{Pipeline Prompts}
\label{app:prompts}

\subsection{Stage A: Claim Extraction Prompt}

\begin{lstlisting}[language=Python]
"For the requirement: [REQUIREMENT TEXT],
identify all relevant claims in the provided
model card. Return extracted text passages that
address this requirement. Be comprehensive---
include all mentions, even brief ones."
\end{lstlisting}

\subsection{Stage B: Scoring Prompt}

\begin{lstlisting}[language=Python]
"Score the following claim on a 0-3 scale:
0 = Not Mentioned
1 = Mentioned (generic)
2 = Partial (some detail)
3 = Thorough (concrete + metrics)

Claim: [CLAIM TEXT]
Requirement: [REQUIREMENT TEXT]
Scoring Guidance: [GUIDANCE]

Provide: (1) score, (2) justification,
(3) exact quote span [character offsets]"
\end{lstlisting}

\section{Model Card Sources}
\label{app:sources}

\begin{itemize}
  \item \textbf{Claude Opus 4.5:} \url{https://assets.anthropic.com/m/.../Claude-Opus-4-5-System-Card.pdf}
  \item \textbf{Gemini 2.5 Pro:} \url{https://deepmind.google/documents/...}
  \item \textbf{Llama 3.1 405B:} \url{https://arxiv.org/pdf/2407.21783}
  \item \textbf{GPT-4o:} \url{https://cdn.openai.com/gpt-4o-system-card.pdf}
  \item \textbf{DeepSeek-R1:} \url{https://arxiv.org/pdf/2501.12948}
\end{itemize}

Download dates: February 2026. URLs current as of report date.

\section{Validation Details}
\label{app:validation}

\subsection{Metrics Breakdown}

Sample size: 80 requirement-score pairs across 3 models

Agreement by framework:
\begin{itemize}
  \item EU CoP: 96.2\% exact agreement
  \item STREAM: 100\% exact agreement
  \item Lab Safety: 100\% exact agreement
\end{itemize}

Agreement by score level:
\begin{itemize}
  \item Score 0: 100\% agreement (small sample, $n=2$)
  \item Score 1: 100\% agreement ($n=18$)
  \item Score 2: 100\% agreement ($n=35$)
  \item Score 3: 100\% agreement ($n=25$)
\end{itemize}

Perfect agreement across all groups suggests rubric clarity and LLM scoring consistency.

\subsection{Disagreement Analysis}

Zero disagreements in this sample. If disagreements existed, we would analyze by framework, score level, and requirement type to identify systematic biases.

\section{Technical Implementation}
\label{app:implementation}

\subsection{Caching}

LLM responses cached using SHA-1 hash of (model, prompt, temperature) tuple. Cache hit rate: 67\% (Stage B reuses extracted claims from Stage A).

\subsection{Concurrency}

100 concurrent API calls (asyncio with semaphore). Rate limit errors handled with exponential backoff. Total runtime: 45 minutes for 400 scores across 2 LLM models.

\subsection{JSON Parsing}

Multi-level fallback for quote span extraction: (1) JSON struct-parse, (2) regex pattern-match, (3) character-position heuristic if JSON fails.

\section{Extended Results: Requirement-Level Scores}
\label{app:extended}

Full 80×5 matrix of scores available in supplementary CSV. Key patterns:

\begin{itemize}
  \item \textbf{Highest-disclosure requirements:} Transparency (CoP-T-*), general safety practices
  \item \textbf{Lowest-disclosure requirements:} Biosafety evaluation results (STREAM-6iii, STREAM-6iv), lab incident response procedures
  \item \textbf{Most variable requirements:} Lab Safety physical security (some models comprehensive, others absent)
\end{itemize}

\section{Limitations of Automated Scoring}
\label{app:limitations}

\subsection{Failure Modes}

\begin{enumerate}
  \item \textbf{Jargon mismatch:} Model card uses different terminology than requirement spec (``frontier safety framework'' vs.\ ``catastrophic risk evaluation'') → LLM might miss relevant claims.

  \item \textbf{Implicit claims:} Some safety practices are implied rather than explicit (``we follow standard lab protocols'') → hard to extract as concrete evidence.

  \item \textbf{Quantitative expectations:} Requirement specifies ``60\% success rate'' but model card says ``majority of tests passed'' → scorer must judge sufficiency.
\end{enumerate}

\subsection{Edge Cases}

\begin{itemize}
  \item Model card references external safety framework (``see supplementary materials'') but external document is not provided → scorer must penalize as Mentioned/1 rather than Thorough/3.

  \item Claim is technically correct but discusses older model version → scorer must judge whether applicable to current model.

  \item Claim is vague enough to be unfalsifiable (``we prioritize safety'') → scores low despite text presence.
\end{itemize}

\section{Data Files}
\label{app:files}

Supplementary materials directory structure:

\begin{lstlisting}
report/
  ├── compliance_leaderboard_report.pdf
  ├── requirements.json (80-requirement rubric)
  ├── scores.json (full 5-model × 80-req scores + evidence)
  ├── leaderboard.csv (rankings)
  ├── validation.csv (human vs auto scores)
  └── figures/ (TikZ source + PDFs)
\end{lstlisting}

\end{document}
