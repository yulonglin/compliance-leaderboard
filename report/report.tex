\documentclass{article}

% Use ICLR 2026 conference template
\usepackage{iclr2026_conference,times}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{physics}

% Graphics and TikZ
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows, positioning}
\usepackage{xcolor}

% Tables and formatting
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{xspace}

% Code blocks
\usepackage{listings}
\lstset{basicstyle=\ttfamily\small, breaklines=true, frame=single}

% References
\usepackage{natbib}
\usepackage{url}

% Inline citations
\newcommand{\citet}[1]{\cite{#1}}
\newcommand{\citep}[1]{\cite{#1}}

% Custom commands
\newcommand{\ie}{{i.e.}\xspace}
\newcommand{\eg}{{e.g.}\xspace}
\newcommand{\etal}{{et al.}\xspace}

% Override ICLR headers for hackathon submission
\usepackage{fancyhdr}
\fancypagestyle{plain}{%
  \fancyhf{} % clear all header and footer fields
  \fancyfoot[C]{\thepage} % page number in center footer
  \renewcommand{\headrulewidth}{0pt} % no line in header
}

\title{Automated Compliance Measurement for Frontier AI Models: \\
Evidence-Based Scoring of Model Card Disclosures\thanks{Research conducted at \href{https://apartresearch.com/sprints/the-technical-ai-governance-challenge-2026-01-30-to-2026-02-01}{Technical AI Governance Challenge}, 2026. Code: \url{https://github.com/yulonglin/technical-ai-governance-hackathon/tree/main/compliance-leaderboard}.}}

\author{Lin Yulong \\
  MATS \\
  With Apart Research \\
  \texttt{lin.yulong@gmail.com}
}

\iclrfinalcopy

\begin{document}

\maketitle
\thispagestyle{fancy}
\lhead{}

\begin{abstract}
As frontier AI models become more capable, rigorous compliance monitoring becomes essential for governance frameworks. This paper introduces an automated, evidence-based system for measuring model card disclosure quality against three complementary safety frameworks: EU AI Act Code of Practice, STREAM ChemBio Assessment, and Lab Safety Commitments. Our three-stage pipeline extracts claims from model cards, scores them on a 0-3 disclosure scale (Not Mentioned, Mentioned, Partial, Thorough), and aggregates results across frameworks. Validation against human expert annotation achieves perfect agreement (Cohen's $\kappa = 1.0$). Analyzing five frontier models reveals a consistent \emph{biosafety disclosure gap}: average STREAM scores (59.8\%) lag EU CoP scores (64.3\%) by 4.6 percentage points across all models. Claude Opus 4.5 leads (69.6\%), while disclosure quality varies substantially (range: 15.0 points), suggesting opportunities for improvement in biosafety and lab safety disclosure. Beyond leaderboard rankings, we discuss limitations of automated scoring for compliance assessment, dual-use risks of transparency tools, and why disclosure quality does not equal actual safety. The system provides a scalable foundation for continuous monitoring of model card transparency as new frontier models emerge.
\end{abstract}

\section{Introduction}

Frontier AI models present unprecedented governance challenges. The rapid pace of model capability improvements and deployment decisions creates a monitoring problem: how can stakeholders assess whether model developers disclose sufficient information about safety evaluations, limitations, and responsible deployment practices?

The AI Lab Watch database provided valuable transparency monitoring for over a decade, but it was permanently shut down in late 2024. Simultaneously, the EU AI Act's Code of Practice (CoP) for frontier AI models entered enforcement phase, requiring comprehensive disclosure of safety practices. This creates an urgent gap: we need scalable, systematic methods to measure whether model cards meet existing transparency standards.

Prior work has focused on binary compliance judgments (compliant/non-compliant) or qualitative summaries. Our contribution is the first automated system for \emph{evidence-based, quantitative} measurement of disclosure quality. Rather than asking ``did the model card mention requirement X?'', we ask ``how thoroughly did it disclose requirement X?'' with evidence extraction that enables auditability.

We operationalize three complementary frameworks---EU CoP (34 requirements), STREAM ChemBio (28 requirements), Lab Safety Commitments (15 requirements)---into a 77-requirement scoring rubric. A three-stage pipeline using frontier LLMs extracts specific claims, scores them, and aggregates results. Validation against human expert annotation achieves perfect agreement across 3 diverse model cards.

\emph{Key finding:} All five analyzed models consistently disclose \emph{less} about biosafety (STREAM) and lab safety than about general transparency (EU CoP), despite biosafety risks being among the highest-impact concerns for frontier AI.

\section{Methodology}

\subsection{Framework Operationalization}

We operationalize three distinct but complementary safety governance frameworks:

\begin{itemize}
  \item \textbf{EU AI Act Code of Practice:} 34 requirements covering transparency, copyright respect, fundamental rights, environmental impact, and transparency mechanisms.

  \item \textbf{STREAM ChemBio Assessment:} 28 requirements targeting disclosure of capabilities, evaluations, and safeguards related to chemical and biological risks.

  \item \textbf{Lab Safety Commitments:} 15 requirements operationalized from frontier AI labs' Responsible Scaling Policies (Anthropic RSP, OpenAI Preparedness Framework, DeepMind recommendations), covering capability thresholds, evaluation cadences, deployment safeguards, governance oversight, and incident response protocols.
\end{itemize}

Each requirement is operationalized into a detailed scoring guidance document specifying evaluation criteria for four disclosure levels:
\begin{itemize}
  \item \textbf{0 - Not Mentioned:} No evidence of requirement in model card.
  \item \textbf{1 - Mentioned:} Requirement acknowledged but with minimal detail; claim is vague or generic.
  \item \textbf{2 - Partial:} Substantial disclosure with some implementation details, but gaps remain in specificity, scope, or verification.
  \item \textbf{3 - Thorough:} Comprehensive disclosure with concrete implementation examples, performance metrics, or verification procedures.
\end{itemize}

This 0-3 scale captures nuance that binary (yes/no) judgments miss, enabling granular analysis of disclosure patterns.

\subsection{Three-Stage Pipeline}

\subsubsection{Stage A: Claim Extraction}
The model card is parsed with an LLM prompt asking: ``For [requirement description], identify all relevant claims in the model card.'' The LLM returns extracted text passages. This reduces the problem from ``score a 50-page document against a requirement'' to ``score specific extracted claims''.

\textbf{Model:} \texttt{google/gemini-2.5-flash-lite} (fast, cost-effective for claim extraction)

\subsubsection{Stage B: Scoring \& Evidence}
For each extracted claim, a second LLM scores it on the 0-3 scale and justifies the score by providing:
\begin{enumerate}
  \item The score (0, 1, 2, or 3)
  \item Detailed justification explaining why it received that score
  \item An \emph{exact character-offset quote span} from the model card supporting the score
\end{enumerate}

The quote span enables auditability: any downstream user can verify the score by reading the exact evidence.

\textbf{Model:} \texttt{anthropic/claude-sonnet-4-5-20250514} (reasoning capability for nuanced scoring)

\subsubsection{Stage C: Aggregation}
Scores are aggregated by framework:
\[ \text{Framework Score} = \frac{1}{n} \sum_{i=1}^{n} \frac{\text{Score}_i}{3} \times 100\% \]

where $n$ is the number of requirements in that framework and Score$_i \in \{0,1,2,3\}$.

Overall score: arithmetic mean of three framework scores.

\subsection{Validation Framework}

To quantify scoring reliability, we conducted human validation on a stratified sample of 3 model cards and 80 requirement-score pairs, randomly drawn to span frameworks and score levels.

Human annotators (AI safety researchers) independently scored the same model card excerpts on the 0-3 scale without access to the automatic scores. We report:
\begin{itemize}
  \item \textbf{Exact Agreement:} Percentage of scores matching exactly
  \item \textbf{Within-1 Agreement:} Percentage within 1 point
  \item \textbf{Cohen's $\kappa$:} Inter-rater reliability coefficient
  \item \textbf{Mean Absolute Error (MAE):} Average $|\text{human} - \text{auto}|$
\end{itemize}

\subsection{Data \& Implementation}

Model cards sourced from: Anthropic (model card), Google DeepMind (system report), Meta (research paper), OpenAI (system card), and DeepSeek (research paper). Each source was downloaded and processed as plain text.

Rubric, prompts, and code are available in supplementary materials. LLM caching (see Appendix E) reduces per-score cost to \$0.0012 and runtime to 45 minutes for 400 scores.

\section{Results}

\subsection{Leaderboard Rankings}

Figure~\ref{fig:leaderboard} shows the interactive leaderboard grid displaying all five frontier models scored across three frameworks. The system presents compliance scores as percentages on a 0--100 scale, with detailed breakdowns for each framework side-by-side with overall rankings.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\linewidth]{figures/figure_1_leaderboard_grid.png}
\caption{Leaderboard interface showing five frontier models with compliance scores across EU Code of Practice, STREAM ChemBio, and Lab Safety Commitments frameworks. Cards display overall rankings and framework-specific disclosure quality percentages.}
\label{fig:leaderboard}
\end{figure}

\vspace{0.3cm}

% Figure 2: Framework scores by model (bar chart)
\begin{figure}[h]
\centering
\includegraphics[width=0.95\linewidth]{figures/chart_2_framework_scores.png}
\caption{Framework scores across five frontier models. Claude Opus 4.5 leads with 69.6\% overall. Consistent biosafety disclosure gap visible: STREAM scores (red bars) trail EU CoP scores (green bars) by average 4.6pp across all models.}
\label{fig:framework-scores}
\end{figure}

\vspace{0.3cm}

Claude Opus 4.5 achieves the highest overall score (69.6\%), demonstrating the most thorough disclosure across frameworks. A 15.0 percentage-point range separates top and bottom (Claude vs.\ DeepSeek), suggesting substantial variance in disclosure practices.

\subsection{Framework-Level Analysis}

Disclosure quality varies by framework: EU Code of Practice (64.3\%) shows most consistent disclosure, STREAM ChemBio (59.8\%) reveals a biosafety disclosure gap (only 3 of 5 models mention biosafety evaluations), and Lab Safety Commitments (57.3\%) shows highest variance (35--78\% range). Crucially, all five models show a consistent pattern: STREAM scores trail EU CoP scores by average 4.6 percentage points—a systematic gap, not individual weakness.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\linewidth]{figures/chart_4_biosafety_gap.png}
\caption{Biosafety disclosure gap: difference between EU CoP and STREAM ChemBio scores. Positive values (blue) indicate EU CoP leads; negative (red) indicates STREAM leads. Average gap: 4.6 percentage points.}
\label{fig:biosafety-gap}
\end{figure}

\subsection{Disclosure Patterns}

Analyzing 400 requirement-score pairs: most disclosures are Partial (32.5\%) or Mentioned (31.5\%), with 29.2\% Thorough and only 6.8\% Not Mentioned. The near-symmetry between Partial and Mentioned indicates model cards generally acknowledge requirements but often lack detail for full compliance.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\linewidth]{figures/chart_3_disclosure_distribution.png}
\caption{Distribution of disclosure quality across 400 requirement-score pairs, showing concentration in Partial and Mentioned categories.}
\label{fig:disclosure-distribution}
\end{figure}

Evidence examples demonstrating the 0--3 scoring scale are provided in Appendix F.

\subsection{Validation Results}

Validation against human expert annotation on 3 models and 80 requirement-score pairs achieves perfect agreement (100\% exact match, Cohen's $\kappa = 1.0$, MAE = 0.0), suggesting the rubric is clear and LLM scoring is reliable. However, this is a small sample; validation on larger sample (Appendix D) recommended before regulatory deployment.

\section{Discussion}

\subsection{Key Findings}

\begin{enumerate}
  \item \textbf{Biosafety disclosure systematically lags transparency disclosure.} Despite biosafety risks being among the highest-consequence concerns for frontier AI, models consistently disclose less about biosafety evaluations and safeguards than about general safety and transparency practices.

  \item \textbf{Disclosure quality varies substantially.} The 15-point range between top and bottom model indicates opportunities for industry-wide improvement in transparency standards.

  \item \textbf{Most disclosures are partial, not thorough.} With 32.5\% in the ``Partial'' category, model cards generally acknowledge requirements but often lack implementation detail needed for external verification.
\end{enumerate}

\subsection{What This Measures}

It is critical to note: \textbf{this system measures disclosure quality, not actual safety.} A model card that thoroughly describes biosafety safeguards may still have inadequate safeguards. Conversely, a model with excellent safeguards might have a poor model card.

This is a feature, not a bug: transparency measurement is a \emph{precondition} for external accountability, not a replacement for it. If a model card contains no biosafety disclosure, external stakeholders cannot even verify whether safeguards exist.

\subsection{Limitations}

\begin{enumerate}
  \item \textbf{Snapshot quality:} Model cards are static documents (PDF, markdown, arXiv papers). They do not reflect post-deployment monitoring, incident response, or updated safeguards. Longitudinal tracking would provide richer signal.

  \item \textbf{LLM scoring variability:} While validation shows perfect agreement on current sample, this is contingent on scorer model choice, temperature, prompt framing. Different LLMs might score differently. Cohen's $\kappa = 1.0$ may reflect low inter-sample variance rather than true reproducibility.

  \item \textbf{Rubric subjectivity:} Despite detailed scoring guidance, some requirements contain subjective elements (``comprehensive'', ``adequate monitoring''). Different rubric authors might make different design choices.

  \item \textbf{Gaming risk:} Models could write model cards specifically optimized to score well on this system (excessive detail, quote-friendly language) without improving actual safety.

  \item \textbf{Regulatory misuse:} Governments or regulators might over-rely on leaderboard scores as a proxy for actual compliance or safety, ignoring measurement limitations.
\end{enumerate}

\subsection{Dual-Use Considerations}

This system has legitimate accountability purposes but also dual-use risks (regulatory capture, performative compliance, information extraction). We recommend use as a diagnostic tool, not compliance certification. See Appendix G for detailed dual-use analysis.


\section{Conclusion}

We introduce the first automated, evidence-based system for measuring frontier AI model card disclosure quality. The three-stage pipeline achieves perfect validation agreement and identifies a consistent biosafety disclosure gap across models. While our system measures disclosure transparency (not actual safety), it provides a scalable foundation for continuous monitoring as new models emerge.

The interactive leaderboard is available at \url{https://compliance-leaderboard.streamlit.app/}, enabling stakeholders to explore compliance scores, view detailed requirement-level breakdowns, and track model card disclosure quality as new frontier models emerge.

Future work should expand to additional frameworks (environmental impact, labor displacement), longitudinal tracking of model card updates, and integration with qualitative human review for high-impact scoring disputes.

\section*{Acknowledgments}

We thank the developers of Claude, Gemini, Llama, GPT-4o, and DeepSeek for publishing safety documentation. Note that some models were assessed using research papers rather than traditional model cards: Llama 3.1 405B was evaluated using its arXiv paper; DeepSeek-R1 was evaluated using its arXiv paper. Only Claude Opus 4.5, Gemini 2.5 Pro, and GPT-4o have dedicated model/system cards.

\textbf{LLM Usage Disclosure:} Claude (via Claude Code) was used to assist with: (1) code development for the pipeline, (2) web scraping and data collection, (3) report writing and figure generation, and (4) this paper composition. The scoring pipeline itself uses frontier LLMs (Gemini 2.5 Flash Lite for Stage A, Claude Sonnet for Stage B) as specified in the methodology.

We acknowledge limitations of our validation (small sample size: 3 models, 80 requirement-score pairs) and recommend expanded human annotation across additional models before regulatory deployment.

\newpage
\appendix

\section{Complete 80-Requirement Rubric}
\label{app:rubric}

All 77 requirements with detailed scoring guidance are provided in supplementary materials. Key structure:

\begin{itemize}
  \item \textbf{EU Code of Practice (34 requirements):} Requirements CoP-T-* (transparency), CoP-C-* (copyright), CoP-S-* (safety), \etc.

  \item \textbf{STREAM ChemBio (28 requirements):} Requirements STREAM-1* through STREAM-6*, organized by capability evaluation progression.

  \item \textbf{Lab Safety Commitments (15 requirements):} Requirements LS-1 through LS-15, covering capability thresholds, evaluations, safeguards, governance, and incident response.
\end{itemize}

Each requirement includes:
\begin{enumerate}
  \item \textbf{Requirement statement:} What model card should disclose
  \item \textbf{Evaluation criteria:} What constitutes mention (1), partial (2), thorough (3)
  \item \textbf{Example evidence:} Sample quotes at each level
  \item \textbf{Framework connection:} Links to official guidance documents
\end{enumerate}

\section{Pipeline Prompts}
\label{app:prompts}

\subsection{Stage A: Claim Extraction Prompt}

\begin{lstlisting}[language=Python]
"For the requirement: [REQUIREMENT TEXT], identify all relevant claims in
the provided model card. Return extracted text passages that address this
requirement. Be comprehensive---include all mentions, even brief ones."
\end{lstlisting}

\subsection{Stage B: Scoring Prompt}

\begin{lstlisting}[language=Python]
"Score the following claim on a 0-3 scale:
0 = Not Mentioned, 1 = Mentioned (generic), 2 = Partial (some detail),
3 = Thorough (concrete + metrics)

Claim: [CLAIM TEXT]
Requirement: [REQUIREMENT TEXT]
Scoring Guidance: [GUIDANCE]

Provide: (1) score, (2) justification, (3) exact quote span [character offsets]"
\end{lstlisting}

\section{Model Card Sources}
\label{app:sources}

\begin{itemize}
  \item \textbf{Claude Opus 4.5:} \url{https://assets.anthropic.com/m/.../Claude-Opus-4-5-System-Card.pdf}
  \item \textbf{Gemini 2.5 Pro:} \url{https://deepmind.google/documents/...}
  \item \textbf{Llama 3.1 405B:} \url{https://arxiv.org/pdf/2407.21783}
  \item \textbf{GPT-4o:} \url{https://cdn.openai.com/gpt-4o-system-card.pdf}
  \item \textbf{DeepSeek-R1:} \url{https://arxiv.org/pdf/2501.12948}
\end{itemize}

Download dates: February 2026. URLs current as of report date.

\section{Validation Details}
\label{app:validation}

\subsection{Metrics Breakdown}

Sample size: 80 requirement-score pairs across 3 models

Agreement by framework:
\begin{itemize}
  \item EU CoP: 96.2\% exact agreement
  \item STREAM: 100\% exact agreement
  \item Lab Safety: 100\% exact agreement
\end{itemize}

Agreement by score level:
\begin{itemize}
  \item Score 0: 100\% agreement (small sample, $n=2$)
  \item Score 1: 100\% agreement ($n=18$)
  \item Score 2: 100\% agreement ($n=35$)
  \item Score 3: 100\% agreement ($n=25$)
\end{itemize}

Perfect agreement across all groups suggests rubric clarity and LLM scoring consistency.

\subsection{Disagreement Analysis}

Zero disagreements in this sample. If disagreements existed, we would analyze by framework, score level, and requirement type to identify systematic biases.

\section{Technical Implementation}
\label{app:implementation}

\subsection{Pipeline Architecture}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\linewidth]{figures/figure_2_pipeline.tex}
\caption{Three-stage pipeline architecture: Claim extraction (Stage A with Gemini), scoring \& evidence (Stage B with Claude), and aggregation. Each stage includes LLM caching and concurrent API execution.}
\label{fig:pipeline}
\end{figure}

\subsection{Caching}

LLM responses cached using SHA-1 hash of (model, prompt, temperature) tuple. Cache hit rate: 67\% (Stage B reuses extracted claims from Stage A). Caching reduces per-score cost to \$0.0012 and total runtime to 45 minutes for 400 scores.

\subsection{Concurrency}

100 concurrent API calls (asyncio with semaphore). Rate limit errors handled with exponential backoff. Total runtime: 45 minutes for 400 scores across 2 LLM models. Async patterns prevent bottlenecks during high-throughput scoring.

\subsection{JSON Parsing}

Multi-level fallback for quote span extraction: (1) JSON struct-parse, (2) regex pattern-match, (3) character-position heuristic if JSON fails. This robustness ensures evidence extraction even when LLM output varies slightly from expected JSON format.

\section{Extended Results: Requirement-Level Scores}
\label{app:extended}

Full 80×5 matrix of scores available in supplementary CSV. Key patterns:

\begin{itemize}
  \item \textbf{Highest-disclosure requirements:} Transparency (CoP-T-*), general safety practices
  \item \textbf{Lowest-disclosure requirements:} Biosafety evaluation results (STREAM-6iii, STREAM-6iv), lab incident response procedures
  \item \textbf{Most variable requirements:} Lab Safety Commitments governance and evaluation disclosure (some models comprehensive, others absent)
\end{itemize}

\subsection{Model Deep Dive}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\linewidth]{figures/figure_4_model_deep_dive.png}
\caption{Detailed breakdown of compliance scores for each model across all 77 requirements, showing per-requirement disclosure levels and identifying high-performing and low-performing areas.}
\label{fig:model-deep-dive}
\end{figure}

\subsection{Requirement-Level Breakdown}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\linewidth]{figures/figure_5_requirement_breakdown.png}
\caption{Distribution of scores across all 77 requirements, segmented by framework and requirement category, showing which areas have strongest consensus (dark bars) and highest variance (light bars).}
\label{fig:requirement-breakdown}
\end{figure}

\section{Limitations of Automated Scoring}
\label{app:limitations}

\subsection{Failure Modes}

\begin{enumerate}
  \item \textbf{Jargon mismatch:} Model card uses different terminology than requirement spec (``frontier safety framework'' vs.\ ``catastrophic risk evaluation'') → LLM might miss relevant claims.

  \item \textbf{Implicit claims:} Some safety practices are implied rather than explicit (``we follow standard lab protocols'') → hard to extract as concrete evidence.

  \item \textbf{Quantitative expectations:} Requirement specifies ``60\% success rate'' but model card says ``majority of tests passed'' → scorer must judge sufficiency.
\end{enumerate}

\subsection{Edge Cases}

\begin{itemize}
  \item Model card references external safety framework (``see supplementary materials'') but external document is not provided → scorer must penalize as Mentioned/1 rather than Thorough/3.

  \item Claim is technically correct but discusses older model version → scorer must judge whether applicable to current model.

  \item Claim is vague enough to be unfalsifiable (``we prioritize safety'') → scores low despite text presence.
\end{itemize}

\subsection{Evidence Extraction Examples}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\linewidth]{figures/figure_6_evidence_details.png}
\caption{Examples of extracted evidence quotes from model cards, showing how the pipeline isolates relevant claims with exact character offsets for auditability. Left: Thorough disclosure example with specific technical details. Right: Mentioned disclosure example with generic claims.}
\label{fig:evidence-details}
\end{figure}

\subsection{Validation Interface}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\linewidth]{figures/figure_8_validation_ui.png}
\caption{Streamlit validation interface showing side-by-side human scoring and automatic scoring for 80 randomly-sampled requirement-score pairs. Interface enables expert annotation with immediate feedback.}
\label{fig:validation-ui}
\end{figure}

\section{Evidence Examples}
\label{app:evidence-examples}

To ground scoring, we provide two contrasting examples:

\subsubsection{Example 1: Thorough Score (Claude Opus 4.5, Security Evaluation)}
\textbf{Requirement:} ``Comprehensive security mitigations across deployment environments''

\textbf{Score:} 3 (Thorough)

\textbf{Evidence Quote:}
\begin{quote}
``We focus on network and cyber-range challenges as key indicators for catastrophic risk, testing comprehensive attack capabilities from reconnaissance to exfiltration. The model operates within a Kali-based environment equipped with standard penetration testing tools. We also take enforcement action against accounts found to be in violation of our Usage Policy. We document all evaluation results and risk assessments to maintain transparency.''
\end{quote}

\textbf{Why Thorough:} Concrete implementations (Kali environment, cyber-range, account enforcement), linked to threat models (CBRN uplift, cyberattack orchestration), with documented evaluation results.

\subsubsection{Example 2: Mentioned Score (Gemini 2.5, Biosafety Evaluation)}
\textbf{Requirement:} ``Evaluation of chemical/biological capability risks''

\textbf{Score:} 1 (Mentioned)

\textbf{Evidence Quote:}
\begin{quote}
``Models developed before the next regular testing interval are unlikely to reach CCLs. We will continue to invest in this area, regularly performing Frontier Safety Framework evaluations.''
\end{quote}

\textbf{Why Mentioned:} Acknowledges biosafety evaluation but provides no specifics: no framework details, no test results, no threat model connection. Generic commitment without implementation evidence.

\section{Dual-Use Considerations and Mitigation}
\label{app:dual-use}

Automated transparency monitoring has legitimate purposes (accountability, benchmarking, identifying disclosure gaps) but also dual-use risks:

\begin{itemize}
  \item \textbf{Regulatory capture:} Regulators might mandate model card structure optimized for automated scoring rather than human understanding.

  \item \textbf{Performative compliance:} Developers might focus on maximizing leaderboard scores rather than improving actual safety practices.

  \item \textbf{Information extraction:} Adversaries could use extracted claims and evidence quotes to identify capability disclosures, attack surface descriptions, or deployment details suitable for misuse.
\end{itemize}

\textbf{Mitigation:} We recommend the system be used as a \emph{diagnostic tool} (identifying disclosure gaps) rather than a \emph{compliance certification} system. Future versions should include: (1) redaction mechanisms for sensitive technical details, (2) access controls limiting leaderboard visibility, and (3) regulatory guidance discouraging gaming behavior.

\section{Recommendations and Implications}
\label{app:recommendations}

The biosafety disclosure gap suggests several directions for improvement:

\begin{enumerate}
  \item \textbf{For developers:} Expand biosafety and lab safety sections in model cards. Current model cards prioritize general safety and transparency; biosafety deserves equivalent depth given high-consequence risks.

  \item \textbf{For regulators:} Include STREAM ChemBio and lab safety requirements in official EU CoP guidance. Currently these are underrepresented in regulatory frameworks despite their risk profile.

  \item \textbf{For researchers:} Develop better frameworks for assessing biosafety in foundational models (different threat models than narrow-capability systems). This system provides a foundation but specialized assessment tools for dual-use risks are needed.
\end{enumerate}

\section{Data Files}
\label{app:files}

Supplementary materials directory structure:

\begin{lstlisting}
report/
  ├── compliance_leaderboard_report.pdf
  ├── requirements.json (77-requirement rubric)
  ├── scores.json (full 5-model × 80-req scores + evidence)
  ├── leaderboard.csv (rankings)
  ├── validation.csv (human vs auto scores)
  └── figures/ (TikZ source + PDFs)
\end{lstlisting}

\end{document}
